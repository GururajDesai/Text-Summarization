{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization_Version_Laura_Guru.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c57gjaTrZAbV",
        "colab_type": "text"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XktyqX5XY-vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import re           \n",
        "from bs4 import BeautifulSoup \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pdb\n",
        "\n",
        "import rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "!pip install py-rouge\n",
        "nltk.download('punkt')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLpg7NCSYtS_",
        "colab_type": "text"
      },
      "source": [
        "## **Import Wikihow Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9Qsz_3vs7ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Text Summarization/wikihowAll.csv\"\n",
        "\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "print(data.head(10))\n",
        "\n",
        "print(data.describe())\n",
        "\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "print(data.describe())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdlqVezqvPR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2R21N_mQprv",
        "colab_type": "text"
      },
      "source": [
        "## **Text Preprocessing**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZuRS_tLFTKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ef118e2-f795-44ef-ae15-00db215bafa5"
      },
      "source": [
        "%%time\n",
        "\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "class PreProcess():\n",
        "  def __init__(self, dataset_path):\n",
        "    self.dataframe = None\n",
        "    self.dataset_path = dataset_path\n",
        "    self.cleaned_dataframe = pd.DataFrame()\n",
        "    self.x_train = None\n",
        "    self.y_train = None\n",
        "    self.x_validation = None\n",
        "    self.y_validation = None\n",
        "    self.x_train_token = None\n",
        "    self.y_train_token = None\n",
        "    self.x_validation_token = None\n",
        "    self.y_validation_token = None\n",
        "    self.x_train_feature = None\n",
        "    self.y_train_label = None\n",
        "    self.x_validation_feature = None\n",
        "    self.y_validation_label = None\n",
        "\n",
        "    self.minimum_word_length = 3\n",
        "    self.max_len_text = 0\n",
        "    self.max_len_summary = 0\n",
        "\n",
        "    self.threshold = 0.8\n",
        "    self.words_description_dataframe = None\n",
        "    self.baseline_model_dataframe = pd.DataFrame()\n",
        "\n",
        "    self.split_percentage = 0.1\n",
        "    self.x_tokenizer = None\n",
        "    self.y_tokenizer = None\n",
        "\n",
        "    print(\"PreProcessing Initialized\")\n",
        "\n",
        "  def import_dataset(self):\n",
        "    print(\"Importing Dataset Started !!!!\")\n",
        "    self.dataframe = pd.read_csv(self.dataset_path)\n",
        "    self.dataframe.dropna(inplace=True)\n",
        "    print(\"Importing Dataset Finished !!!!\")\n",
        "\n",
        "  def print_original_data(self):\n",
        "    print(self.dataframe.head(3))\n",
        "    \n",
        "  def print_processed_data(self):\n",
        "    print(self.cleaned_dataframe.head(3))\n",
        "\n",
        "  def return_cleaned_dataframe(self):\n",
        "    return self.cleaned_dataframe\n",
        "\n",
        "  def remove_stopwords(self, text):\n",
        "    return [w for w in text.split() if not w in stop_words]\n",
        "\n",
        "  def parse_html_text(self, text):\n",
        "    return BeautifulSoup(text, \"lxml\").text\n",
        "\n",
        "  def remove_special_charecters(self, text):\n",
        "    return re.sub(r'\\([^)]*\\)', '', text)\n",
        "\n",
        "  def remove_double_quotes(self, text):\n",
        "    return re.sub('\"','', text)\n",
        "\n",
        "  def map_different_words(self, text):\n",
        "    return' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")]) \n",
        "\n",
        "  def filter_only_text(self, text):\n",
        "    text = re.sub(r\"'s\\b\",\"\",text)\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    return text\n",
        "  \n",
        "  def give_text_with_punct(self, text):\n",
        "    alphanumeric = re.sub( '[^a-z0-9.]', ' ', text)\n",
        "    return alphanumeric\n",
        "\n",
        "  def clean_text(self, text):\n",
        "\n",
        "    lower_text = text.lower()\n",
        "\n",
        "    parsed_html_text = self.parse_html_text(lower_text)\n",
        "\n",
        "    special_charecters_removed_text = self.remove_special_charecters(parsed_html_text)\n",
        "\n",
        "    double_quotes_removed_text = self.remove_double_quotes(special_charecters_removed_text)\n",
        "\n",
        "    mapped_text = self.map_different_words(double_quotes_removed_text)\n",
        "       \n",
        "    filtered_text = self.filter_only_text(mapped_text)\n",
        "\n",
        "    tokens = self.remove_stopwords(filtered_text)\n",
        "\n",
        "    sentence=[]\n",
        "    for token in tokens:\n",
        "        if len(token)>=self.minimum_word_length:\n",
        "            sentence.append(token)\n",
        "\n",
        "    return (\" \".join(sentence)).strip()\n",
        "\n",
        "  def clean_text_withp(self, text):\n",
        "\n",
        "    lower_text = text.lower()\n",
        "\n",
        "    parsed_html_text = self.parse_html_text(lower_text)\n",
        "\n",
        "    special_charecters_removed_text = self.remove_special_charecters(parsed_html_text)\n",
        "\n",
        "    double_quotes_removed_text = self.remove_double_quotes(special_charecters_removed_text)\n",
        "\n",
        "    filtered_with_punct_text = self.give_text_with_punct(double_quotes_removed_text)\n",
        "    \n",
        "    mapped_text = self.map_different_words(filtered_with_punct_text)\n",
        "       \n",
        "    tokens = self.remove_stopwords(mapped_text)\n",
        "\n",
        "    sentence=[]\n",
        "    for token in tokens:\n",
        "        if len(token)>=self.minimum_word_length:\n",
        "            sentence.append(token)\n",
        "    return (\" \".join(sentence)).strip()\n",
        "\n",
        "\n",
        "  def create_cleaned_dataframe(self):\n",
        "    print(\"Dataframe text cleaning started!!!\")\n",
        "    cleaned_text = []\n",
        "    for t in self.dataframe.text:\n",
        "        cleaned_text.append(self.clean_text_withp(t))\n",
        "        \n",
        "    cleaned_title = []\n",
        "    for t in self.dataframe.title:\n",
        "        cleaned_title.append(self.clean_text_withp(t))\n",
        "    \n",
        "    self.cleaned_dataframe[\"cleaned_text\"] = cleaned_text\n",
        "    self.cleaned_dataframe[\"cleaned_summary\"] = cleaned_title\n",
        "    self.cleaned_dataframe['cleaned_summary'].replace('', np.nan, inplace=True)\n",
        "    self.cleaned_dataframe.dropna(axis=0,inplace=True)\n",
        "    print(\"Dataframe text cleaning finished!!!\")\n",
        "\n",
        "  def calculate_word_length_ratio(self):\n",
        "    self.words_description_dataframe[\"ratio_summary/text\"] = self.words_description_dataframe[\"summary\"] / self.words_description_dataframe[\"text\"]\n",
        "    self.words_description_dataframe[\"ratio_text/summary\"] = self.words_description_dataframe[\"text\"] / self.words_description_dataframe[\"summary\"]\n",
        "    self.words_description_dataframe = self.words_description_dataframe.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "  def update_maximum_length_of_text_and_summary(self):\n",
        "    print(\"Finding maximum length of text and summary started!!!\")\n",
        "    text_word_count = []\n",
        "    summary_word_count = []\n",
        "\n",
        "    for i in self.cleaned_dataframe['cleaned_text']:\n",
        "          text_word_count.append(len(i.split()))\n",
        "\n",
        "    for i in self.cleaned_dataframe['cleaned_summary']:\n",
        "          summary_word_count.append(len(i.split()))\n",
        "\n",
        "    self.words_description_dataframe = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "    self.max_len_text=self.words_description_dataframe.describe().text[\"max\"] \n",
        "    self.max_len_summary=self.words_description_dataframe.describe().summary[\"max\"]\n",
        "\n",
        "    self.max_len_text = int(self.max_len_text)\n",
        "    self.max_len_summary = int(self.max_len_summary)\n",
        "    print(\"Finding maximum length of text and summary finished!!!\")\n",
        "\n",
        "\n",
        "  def split_dataset(self):\n",
        "    print(\"Splitting of dataset into train and text started!!!\")\n",
        "    self.x_train,self.x_validation,self.y_train,self.y_validation = train_test_split(\n",
        "        self.cleaned_dataframe['cleaned_text'],self.cleaned_dataframe['cleaned_summary'],test_size=self.split_percentage,\n",
        "                                          random_state=0,shuffle=True)\n",
        "    print(\"Splitting of dataset into train and text finished!!!\")\n",
        "\n",
        "  def tokenize(self):\n",
        "    print(\"tokenization of text data started!!!\")\n",
        "    self.x_tokenizer = Tokenizer()\n",
        "\n",
        "    self.x_tokenizer.fit_on_texts(list(self.x_train))\n",
        "\n",
        "    self.x_train_token = self.x_tokenizer.texts_to_sequences(self.x_train) \n",
        "    self.x_validation_token = self.x_tokenizer.texts_to_sequences(self.x_validation)\n",
        "\n",
        "    self.y_tokenizer = Tokenizer()\n",
        "    self.y_tokenizer.fit_on_texts(list(self.y_train))\n",
        "\n",
        "    self.y_train_token = self.y_tokenizer.texts_to_sequences(self.y_train) \n",
        "    self.y_validation_token = self.y_tokenizer.texts_to_sequences(self.y_validation)\n",
        "\n",
        "    print(\"tokenization of text data finished!!!\")\n",
        "\n",
        "  def convert_tokens_to_sequences(self):\n",
        "    print(\"converting tokens into sequences started!!!\")\n",
        "\n",
        "    self.x_train_feature = pad_sequences(self.x_train_token, maxlen=self.max_len_text, padding='post')\n",
        "    self.x_validation_feature = pad_sequences(self.x_validation_token, maxlen=self.max_len_text, padding='post')\n",
        "\n",
        "    self.x_vocabulary_size   =  len(self.x_tokenizer.word_index) +1\n",
        "\n",
        "    self.y_train_label = pad_sequences(self.y_train_token, maxlen=self.max_len_summary, padding='post')\n",
        "    self.y_validation_label = pad_sequences(self.y_validation_token, maxlen=self.max_len_summary, padding='post')\n",
        "\n",
        "    self.y_vocabulary_size  =   len(self.y_tokenizer.word_index) +1\n",
        "\n",
        "    print(\"converting tokens into sequences finished!!!\")\n",
        "\n",
        "\n",
        "  def create_word_index_from_tokens(self):\n",
        "      self.reverse_target_word_index=self.y_tokenizer.index_word \n",
        "      self.reverse_source_word_index=self.x_tokenizer.index_word \n",
        "      self.target_word_index=self.y_tokenizer.word_index\n",
        "      self.reverse_target_word_index[0] = \"UNKNOWN\"\n",
        "\n",
        "  def assign_ratios_to_cleaned_dataframe(self):\n",
        "    self.cleaned_dataframe[\"ratio_text/summary\"] = self.words_description_dataframe[\"ratio_text/summary\"]\n",
        "    self.cleaned_dataframe[\"ratio_summary/text\"] = self.words_description_dataframe[\"ratio_summary/text\"]\n",
        "\n",
        "  def filter_dataframe_with_threshold(self):\n",
        "    print(\"filtering dataframe with threshold has started!!!\")\n",
        "    self.cleaned_dataframe = self.cleaned_dataframe[self.cleaned_dataframe[\"ratio_summary/text\"] < self.threshold]\n",
        "    print(\"filtering dataframe with threshold has finished!!!\")\n",
        "\n",
        "\n",
        "  def word_frequency(self, dataset):\n",
        "    pred_summary_sentence_list = []\n",
        "    actual_summary_sentence_list = []\n",
        "    i = 0\n",
        "    for index, row in dataset.iterrows():\n",
        "      try:\n",
        "        sentence = row[0]\n",
        "        summary = row[1]\n",
        "\n",
        "        sentence_list = nltk.sent_tokenize(sentence)\n",
        "        word_frequencies = {}\n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            if word not in stop_words:\n",
        "                if word not in word_frequencies.keys():\n",
        "                    word_frequencies[word] = 1\n",
        "                else:\n",
        "                    word_frequencies[word] += 1\n",
        "\n",
        "        maximum_frequency = max(word_frequencies.values(), default=1)\n",
        "        for word in word_frequencies.keys():\n",
        "              word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
        "\n",
        "        sentence_scores = {}\n",
        "        for sent in sentence_list:\n",
        "            for word in nltk.word_tokenize(sent.lower()):\n",
        "                if word in word_frequencies.keys():\n",
        "                    if len(sent.split(' ')) < 5:\n",
        "                        if sent not in sentence_scores:\n",
        "                            sentence_scores[sent] = word_frequencies[word]\n",
        "                        else:\n",
        "                            sentence_scores[sent] += word_frequencies[word]\n",
        "\n",
        "        sorted_scores = sorted(sentence_scores.items(), key=lambda f: f[1])\n",
        "        sentence_summary = None\n",
        "        if(len(sorted_scores) > 0):\n",
        "          sentence_summary = sorted_scores[-1][0]\n",
        "        else:\n",
        "          sentence_summary = \"\"\n",
        "\n",
        "        pred_summary_sentence_list.append(sentence_summary)\n",
        "        actual_summary_sentence_list.append(summary)\n",
        "        for key in sentence_scores:\n",
        "          summary_sentences = heapq.nlargest(1, sentence_scores, key=sentence_scores.get)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        pass\n",
        "      i = i + 1\n",
        "    return pred_summary_sentence_list, actual_summary_sentence_list\n",
        "    \n",
        "  def create_word_freq(self):\n",
        "    print(\"Create word frequency has started :-)\")\n",
        "    self.predicted_summaries, self.actual_summaries = self.word_frequency(self.cleaned_dataframe)\n",
        "    print(\"Create word frequency has finished :-)\")\n",
        "\n",
        "  def assign_text_to_baseline_df(self):\n",
        "    print(\"Create baseline dataframe has started :-)\")\n",
        "    self.baseline_model_dataframe[\"actual_summaries\"] = self.actual_summaries\n",
        "    self.baseline_model_dataframe[\"predicted_summaries\"] = self.predicted_summaries\n",
        "    self.baseline_model_dataframe[\"text\"] = self.cleaned_dataframe[\"cleaned_text\"]\n",
        "    print(\"Create baseline dataframe has finished :-)\")\n",
        "\n",
        "  def process(self):\n",
        "    print(\"Preprocessing Started :-)\")\n",
        "    self.import_dataset()\n",
        "    self.create_cleaned_dataframe()\n",
        "    self.update_maximum_length_of_text_and_summary()\n",
        "    self.calculate_word_length_ratio()\n",
        "    self.assign_ratios_to_cleaned_dataframe()\n",
        "    self.filter_dataframe_with_threshold()\n",
        "    self.split_dataset()\n",
        "    self.tokenize()\n",
        "    self.convert_tokens_to_sequences()\n",
        "    print(\"Preprocessing Finished :-)\")\n",
        "\n",
        "  def baseline(self):\n",
        "    print(\"Baseline Started :-)\")\n",
        "    self.import_dataset()\n",
        "    self.create_cleaned_dataframe()\n",
        "    self.update_maximum_length_of_text_and_summary()\n",
        "    self.calculate_word_length_ratio()\n",
        "    self.assign_ratios_to_cleaned_dataframe()\n",
        "    self.filter_dataframe_with_threshold()\n",
        "    self.split_dataset()\n",
        "    self.tokenize()\n",
        "    self.convert_tokens_to_sequences()\n",
        "    self.create_word_freq()\n",
        "    self.assign_text_to_baseline_df()\n",
        "    print(\"Baseline Finished :-)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.49 ms, sys: 2 µs, total: 1.49 ms\n",
            "Wall time: 1.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOq6LigNJUww",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8cd5499d-630b-4135-b688-976e76387b8c"
      },
      "source": [
        "#@title\n",
        "#%%time\n",
        "preprocess_object = PreProcess(path)\n",
        "preprocess_object.process()\n",
        "#preprocess_object.print_original_data()\n",
        "#preprocess_object.print_processed_data()\n",
        "preprocess_object.create_word_index_from_tokens()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PreProcessing Initialized\n",
            "Preprocessing Started :-)\n",
            "Importing Dataset Started !!!!\n",
            "Importing Dataset Finished !!!!\n",
            "Dataframe text cleaning started!!!\n",
            "Dataframe text cleaning finished!!!\n",
            "Finding maximum length of text and summary started!!!\n",
            "Finding maximum length of text and summary finished!!!\n",
            "filtering dataframe with threshold has started!!!\n",
            "filtering dataframe with threshold has finished!!!\n",
            "Splitting of dataset into train and text started!!!\n",
            "Splitting of dataset into train and text finished!!!\n",
            "tokenization of text data started!!!\n",
            "tokenization of text data finished!!!\n",
            "converting tokens into sequences started!!!\n",
            "converting tokens into sequences finished!!!\n",
            "Preprocessing Finished :-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuJBO0dQz_gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3107c03a-dd9d-4735-c3dd-a9b73a1057e7"
      },
      "source": [
        "preprocess_object.x_train_feature.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(185587, 6529)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA_XBToO9E6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e031ecb-cdf4-4c10-bed4-6c2c5278223f"
      },
      "source": [
        "preprocess_object.y_train_label.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(185587, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxMu9CWC8RL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d66a979e-11b6-4682-8703-032b5299010c"
      },
      "source": [
        "preprocess_object.dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\nKeep related supplies in the same area.,\\nMake an effort to clean a dedicated workspace after every session.,\\nPlace loose supplies in large, clearly visible containers.,\\nUse clotheslines and c...</td>\n",
              "      <td>How to Be an Organized Artist1</td>\n",
              "      <td>If you're a photographer, keep all the necessary lens, cords, and batteries in the same quadrant of your home or studio. Paints should be kept with brushes, cleaner, and canvas, print supplies sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\nCreate a sketch in the NeoPopRealist manner of the future mural on a small piece of paper 8\"x10\" using the black ink pen.,\\nPrepare to create your NeoPopRealist mural.,\\nPrepare your paint.,\\nBe...</td>\n",
              "      <td>How to Create a Neopoprealist Art Work</td>\n",
              "      <td>See the image for how this drawing develops step-by-step. However, there is an important detail: the following drawings are to examine it, and then, to create something unique.\\n\\n\\nUse the lines...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nGet a bachelor’s degree.,\\nEnroll in a studio-based program.,\\nTrain on a number of VFX computer programs.,\\nWatch online tutorials.,\\nNurture your artistic side.,\\nPay close attention to movies...</td>\n",
              "      <td>How to Be a Visual Effects Artist1</td>\n",
              "      <td>It is possible to become a VFX artist without a college degree, but the path is often easier with one. VFX artists usually major in fine arts, computer graphics, or animation. Choose a college wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\nStart with some experience or interest in art.,\\nUnderstand the difference between art collectors, art investors and art speculators.,\\nFigure out what you are willing to pay for art, before goi...</td>\n",
              "      <td>How to Become an Art Investor</td>\n",
              "      <td>The best art investors do their research on the pieces of art that they buy, so someone with some education or interest in the art world is more likely to understand this niche market. As well as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\nKeep your reference materials, sketches, articles, photos, etc, in one easy to find place.,\\nMake \"studies,\" or practice sketches, to organize effectively for larger projects.,\\nLimit the suppli...</td>\n",
              "      <td>How to Be an Organized Artist2</td>\n",
              "      <td>As you start planning for a project or work, you'll likely be gathering scraps of inspiration and test sketches. While everyone has a strategy, there is nothing more maddening than digging throug...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                  headline  ...                                                                                                                                                                                                     text\n",
              "0  \\nKeep related supplies in the same area.,\\nMake an effort to clean a dedicated workspace after every session.,\\nPlace loose supplies in large, clearly visible containers.,\\nUse clotheslines and c...  ...   If you're a photographer, keep all the necessary lens, cords, and batteries in the same quadrant of your home or studio. Paints should be kept with brushes, cleaner, and canvas, print supplies sh...\n",
              "1  \\nCreate a sketch in the NeoPopRealist manner of the future mural on a small piece of paper 8\"x10\" using the black ink pen.,\\nPrepare to create your NeoPopRealist mural.,\\nPrepare your paint.,\\nBe...  ...   See the image for how this drawing develops step-by-step. However, there is an important detail: the following drawings are to examine it, and then, to create something unique.\\n\\n\\nUse the lines...\n",
              "2  \\nGet a bachelor’s degree.,\\nEnroll in a studio-based program.,\\nTrain on a number of VFX computer programs.,\\nWatch online tutorials.,\\nNurture your artistic side.,\\nPay close attention to movies...  ...   It is possible to become a VFX artist without a college degree, but the path is often easier with one. VFX artists usually major in fine arts, computer graphics, or animation. Choose a college wi...\n",
              "3  \\nStart with some experience or interest in art.,\\nUnderstand the difference between art collectors, art investors and art speculators.,\\nFigure out what you are willing to pay for art, before goi...  ...   The best art investors do their research on the pieces of art that they buy, so someone with some education or interest in the art world is more likely to understand this niche market. As well as...\n",
              "4  \\nKeep your reference materials, sketches, articles, photos, etc, in one easy to find place.,\\nMake \"studies,\" or practice sketches, to organize effectively for larger projects.,\\nLimit the suppli...  ...   As you start planning for a project or work, you'll likely be gathering scraps of inspiration and test sketches. While everyone has a strategy, there is nothing more maddening than digging throug...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eNYbogC8KV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "661553aa-0f5e-4545-e5e6-ca423b4b9e4b"
      },
      "source": [
        "preprocess_object.cleaned_dataframe.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ratio_text/summary</th>\n",
              "      <th>ratio_summary/text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>206208.000000</td>\n",
              "      <td>206208.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>77.565887</td>\n",
              "      <td>0.052816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>101.953947</td>\n",
              "      <td>0.094736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.285714</td>\n",
              "      <td>0.000321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>21.666667</td>\n",
              "      <td>0.010929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>47.333333</td>\n",
              "      <td>0.021127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>91.500000</td>\n",
              "      <td>0.046154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3119.000000</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ratio_text/summary  ratio_summary/text\n",
              "count       206208.000000       206208.000000\n",
              "mean            77.565887            0.052816\n",
              "std            101.953947            0.094736\n",
              "min              1.285714            0.000321\n",
              "25%             21.666667            0.010929\n",
              "50%             47.333333            0.021127\n",
              "75%             91.500000            0.046154\n",
              "max           3119.000000            0.777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cix6bU5pUMhN",
        "colab_type": "text"
      },
      "source": [
        "## **Baseline Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e9F9l4SUIa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocess_object = PreProcess(path)\n",
        "preprocess_object.process()\n",
        "\n",
        "actual_summaries, predicted_summaries = preprocess_object.word_frequency()\n",
        "\n",
        "baseline_model_dataframe = pd.DataFrame()\n",
        "baseline_model_dataframe[\"actual_summaries\"] = actual_summaries\n",
        "baseline_model_dataframe[\"predicted_summaries\"] = predicted_summaries\n",
        "baseline_model_dataframe[\"text\"] = preprocess_object.cleaned_dataframe.iloc[0:dataset_length,][\"cleaned_text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyzGK2JiWv8Z",
        "colab_type": "text"
      },
      "source": [
        "## **Baseline Model Evaluation**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMi0i5mpW9bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import rouge\n",
        "\n",
        "class Evaluation:\n",
        "  def __init__(self, name, actual_summaries, predicted_summaries):\n",
        "    self.name = \"rouge\"\n",
        "    self.actual_summaries = actual_summaries\n",
        "    self.predicted_summaries = predicted_summaries\n",
        "    self.synonym_summaries = None\n",
        "\n",
        "  def rouge_score(self):\n",
        "    aggregator = \"Best\"\n",
        "    apply_avg = aggregator == 'Avg'\n",
        "    apply_best = aggregator == 'Best'\n",
        "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
        "                           max_n=4,\n",
        "                           limit_length=True,\n",
        "                           length_limit=100,\n",
        "                           length_limit_type='words',\n",
        "                           apply_avg=apply_avg,\n",
        "                           apply_best=apply_best,\n",
        "                           alpha=0.5, # Default F1_score\n",
        "                           weight_factor=1.2,\n",
        "                           stemming=True)\n",
        "    scores = evaluator.get_scores(predicted_summaries, actual_summaries)\n",
        "    print(scores)\n",
        "    return scores\n",
        "\n",
        "  def evaluate(self):\n",
        "    if self.name == \"rouge\":\n",
        "      self.rouge_score()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBUMHp2xXEzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_object = Evaluation(\"rouge\", actual_summaries, predicted_summaries)\n",
        "\n",
        "evaluation_object.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHCDDlMcQ_O5",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfD7k-vnQT6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextSummarizatioModel():\n",
        "  def __init__(self):\n",
        "    self.model = None\n",
        "    self.encoded_feature_dimension = 350\n",
        "    # self.model_weight_path = \"/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/refactored_code_model_weights.hd5\"\n",
        "    self.model_weight_path = \"/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/final_model_version_3_weights_epoch_28.hd5\"\n",
        "    # self.saved_model_path = \"/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/refactored_code_model.hd5\"\n",
        "    self.saved_model_path = \"/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/small_refactored_code_model_temp.hd5\"\n",
        "    #self.load_model_weights_path = \"/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/200000_dataset_model_with_256_batch_and_3_epoch.hd5\"\n",
        "    #self.load_model_weights_path = \"/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/small_refactored_code_model_weights.hd5\"\n",
        "    self.load_model_weights_path = \"/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/final_model_version_3_weights_epoch_28.hd5\"\n",
        "\n",
        "    self.tpu_model_path = \"/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/tpu_mode.hd5\"\n",
        "    self.epochs = 20\n",
        "    self.batch_size = 256\n",
        "    self.history = None\n",
        "\n",
        "  def print_model_summary(self):\n",
        "    self.print_asterix()\n",
        "    self.model.summary()\n",
        "    self.print_asterix()\n",
        "\n",
        "  def print_asterix(self):\n",
        "    print(\"***\" * 30)\n",
        "\n",
        "  def create_model(self):\n",
        "    print(\"***\" * 30)\n",
        "    print(\"Creating Model Started!!!!\")\n",
        "    K.clear_session()\n",
        "\n",
        "    # Encodeing Layer\n",
        "    self.encoder_inputs = Input(shape=(preprocess_object.max_len_text,)) \n",
        "\n",
        "    # add Embedding layer, each word will have embedding vector of size encoded_feature_dimensions\n",
        "    self.enc_emb = Embedding(preprocess_object.x_vocabulary_size, \n",
        "                             self.encoded_feature_dimension,trainable=True)(self.encoder_inputs) \n",
        "\n",
        "\n",
        "    self.encoder_lstm1 = LSTM(self.encoded_feature_dimension,return_sequences=True,return_state=True) \n",
        "    self.encoder_output1, self.state_h1, self.state_c1 = self.encoder_lstm1(self.enc_emb) \n",
        "\n",
        "    self.encoder_lstm2=LSTM(self.encoded_feature_dimension, return_state=True, return_sequences=True) \n",
        "    self.encoder_outputs, self.state_h, self.state_c= self.encoder_lstm2(self.encoder_output1) \n",
        "\n",
        "    #Decoding Layer\n",
        "    self.decoder_inputs = Input(shape=(None,)) \n",
        "    self.dec_emb_layer = Embedding(preprocess_object.y_vocabulary_size, self.encoded_feature_dimension,trainable=True) \n",
        "    self.dec_emb = self.dec_emb_layer(self.decoder_inputs) \n",
        "\n",
        "    self.decoder_lstm = LSTM(self.encoded_feature_dimension, return_sequences=True, return_state=True) \n",
        "    self.decoder_outputs,self.decoder_fwd_state, self.decoder_back_state = self.decoder_lstm(\n",
        "        self.dec_emb,initial_state=[self.state_h, self.state_c]) \n",
        "\n",
        "    self.decoder_dense = TimeDistributed(Dense(preprocess_object.y_vocabulary_size, activation='softmax'))\n",
        "    self.decoder_outputs = self.decoder_dense(self.decoder_outputs) \n",
        "\n",
        "    self.model = Model([self.encoder_inputs, self.decoder_inputs], self.decoder_outputs) \n",
        "    self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "    print(\"Creating Model Finished!!!!\")\n",
        "    print(\"***\" * 30)\n",
        "\n",
        "  def train(self, x_train_feature, x_validation_feature, y_train_feature, y_validation_feature):\n",
        "    self.print_asterix()\n",
        "    print(\"Model training started!!!!\")\n",
        "    #compile the model\n",
        "    self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "    #add early stopping to save intermediate states\n",
        "    self.es = EarlyStopping(monitor='loss', mode='min', verbose=1)\n",
        "    self.checkpoint = ModelCheckpoint(self.model_weight_path, save_best_only=True, monitor='loss', mode='min')\n",
        "\n",
        "\n",
        "    self.history=self.model.fit([x_train_feature,y_train_feature[:,:-1]], \n",
        "                           y_train_feature.reshape(y_train_feature.shape[0],\n",
        "                                                   y_train_feature.shape[1], 1)[:,1:] ,\n",
        "                    epochs=self.epochs,callbacks=[self.es, self.checkpoint],batch_size=self.batch_size)\n",
        "\n",
        "    print(\"Model training Finished!!!!\")\n",
        "    self.print_asterix()\n",
        "\n",
        "  def save_model(self):\n",
        "    self.print_asterix()\n",
        "    print(\"Saving Model!!!!!!!!!!\")\n",
        "    self.model.save(self.saved_model_path)\n",
        "    print(\"Model saved successfully!!!!!!!!!!\")\n",
        "    self.print_asterix()\n",
        "\n",
        "  def load_model(self):\n",
        "    self.print_asterix()\n",
        "    print(\"Loading Model started!!!!!!!!!!\")\n",
        "    self.model.load_weights(self.load_model_weights_path)\n",
        "    print(\"Loading Model finished!!!!!!!!!!\")\n",
        "    self.print_asterix()\n",
        "\n",
        "  def print_train_history(self): \n",
        "    self.print_asterix()\n",
        "    plt.title(\"Model Trained History\")\n",
        "    plt.plot(self.history.history['loss'], label='train') \n",
        "    plt.plot(self.history.history['val_loss'], label='test') \n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    self.print_asterix()\n",
        "\n",
        "  def evaluation_model(self):\n",
        "    self.print_asterix()\n",
        "    print(\"Creating Evaluation Model Started!!!\")\n",
        "    self.encoder_model = Model(inputs= self.encoder_inputs,outputs=[self.encoder_outputs, self.state_h, self.state_c])\n",
        "  \n",
        "    decoder_state_input_h = Input(shape=(self.encoded_feature_dimension,))\n",
        "    decoder_state_input_c = Input(shape=(self.encoded_feature_dimension,))\n",
        "    decoder_hidden_state_input = Input(shape=(preprocess_object.max_len_text,self.encoded_feature_dimension))\n",
        "    dec_emb2= self.dec_emb_layer(self.decoder_inputs)\n",
        "\n",
        "    decoder_outputs2, state_h2, state_c2 = self.decoder_lstm(dec_emb2, \n",
        "                                                        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "    decoder_inf_concat = decoder_outputs2\n",
        "    decoder_outputs2 = self.decoder_dense(decoder_inf_concat)\n",
        "\n",
        "    self.decoder_model = Model([self.decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, \n",
        "                                                        decoder_state_input_c],[decoder_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "    print(\"Creating Evaluation Model Finished!!!\")\n",
        "    self.print_asterix()\n",
        "\n",
        "  def decode_sequence(self, input_seq):\n",
        "    # Use greedy method to find the next predicted words\n",
        "    # Take the index of the vocabulary from the maximum probability value of the predicted output from the decoder\n",
        "    # convert the index to word using the vocabulary dictionary built earlier\n",
        "    # returns the output in the text form\n",
        "\n",
        "    e_out, e_h, e_c = self.encoder_model.predict(input_seq)\n",
        "\n",
        "    #print(\"Encoder Output: {}\".format(e_out))\n",
        "\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # first input should be start for the decoder model so that it will start to predict the next words based on start and hidden states from the encoders\n",
        "    target_seq[0, 0] = preprocess_object.target_word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    i = 0\n",
        "    while not stop_condition:\n",
        "        i = i + 1\n",
        "        #print(\"Iteration : {}\".format(i))\n",
        "\n",
        "        # print(\"Input to decoder: {}\".format([[target_seq] + [e_out, e_h, e_c]]))\n",
        "\n",
        "        output_tokens, h, c = self.decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        #print(\"Decoder output: {}\".format(output_tokens))\n",
        "\n",
        "        # pdb.set_trace()\n",
        "        # print(output_tokens)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        #print(\"Sampled Token Index: {}\".format(sampled_token_index))\n",
        "        \n",
        "        # if sampled_token_index == 0 and i == 25:\n",
        "        if sampled_token_index == 0:\n",
        "          stop_condition = True\n",
        "          continue\n",
        "        \n",
        "        sampled_token = preprocess_object.reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='end'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (preprocess_object.max_len_summary-1)):\n",
        "                #print(\"Decode sentence: {}\".format(decoded_sentence))\n",
        "                print(len(decoded_sentence.split()))\n",
        "                stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "  def seq2summary(self, input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=preprocess_object.target_word_index['start']) and i!=preprocess_object.target_word_index['end']):\n",
        "        newString=newString+preprocess_object.reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "  def seq2text(self, input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i!=0):\n",
        "        newString=newString+preprocess_object.reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThURIZqugU_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b06378c-b96d-4193-d664-b3386e69b28f"
      },
      "source": [
        "np.zeros((1,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebIuiklbRK67",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vTCCM2KG3X6D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "1f62a1e1-c05a-44b3-a4ea-0ac9f458d5b6"
      },
      "source": [
        "#%%time\n",
        "text_summary_model = TextSummarizatioModel()\n",
        "text_summary_model.create_model()\n",
        "text_summary_model.load_model()\n",
        "text_summary_model.print_model_summary()\n",
        "text_summary_model.train(preprocess_object.x_train_feature, preprocess_object.x_validation_feature, \n",
        "                       preprocess_object.y_train_label, preprocess_object.y_validation_label)\n",
        "text_summary_model.save_model()\n",
        "text_summary_model.evaluation_model()\n",
        "text_summary_model.print_train_history()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************************************************************************************\n",
            "Creating Model Started!!!!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Creating Model Finished!!!!\n",
            "******************************************************************************************\n",
            "******************************************************************************************\n",
            "Loading Model started!!!!!!!!!!\n",
            "Loading Model finished!!!!!!!!!!\n",
            "******************************************************************************************\n",
            "******************************************************************************************\n",
            "Creating Evaluation Model Started!!!\n",
            "Creating Evaluation Model Finished!!!\n",
            "******************************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzfpd2PCy2cA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fedbf9e6-64bd-4449-9921-af746d930e2e"
      },
      "source": [
        "text_summary_model.evaluation_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************************************************************************************\n",
            "Creating Evaluation Model Started!!!\n",
            "Creating Evaluation Model Finished!!!\n",
            "******************************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOT0xvnwqzLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "975239b9-199d-4818-f62d-7a6a40595e16"
      },
      "source": [
        "preprocess_object.x_validation_feature.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20621"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D4jXV7VLKbt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clZGHqgnRQMX",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM Model Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0NS6olLt5Rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length = 3500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DHKHDEl-OQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "c6a2ad05-ce35-40eb-a04e-56e544a0f645"
      },
      "source": [
        "%%time\n",
        "\n",
        "import csv\n",
        "with open('/content/drive/My Drive/Leuphana Projects/Text Summarization/ModelWeights/predicted_summaries.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    predicted_summaries = []\n",
        "    actual_summaries = []\n",
        "\n",
        "    for i in range(1925, length):\n",
        "\n",
        "      if(i%100 == 0):\n",
        "        print(i)\n",
        "\n",
        "      #print(\"Review:\",text_summary_model.seq2text(preprocess_object.x_validation_feature[i]))\n",
        "\n",
        "      #print(\"Original summary:\",text_summary_model.seq2summary(preprocess_object.y_validation_label[i]))\n",
        "\n",
        "      acutual_summary = text_summary_model.seq2summary(preprocess_object.y_validation_label[i])\n",
        "      actual_summaries.append(acutual_summary)\n",
        "\n",
        "      #print(\"Predicted summary: \",text_summary_model.decode_sequence(\n",
        "      #   preprocess_object.x_validation_feature[i].reshape(1,preprocess_object.max_len_text)))\n",
        "      \n",
        "      predicted_summary = text_summary_model.decode_sequence(preprocess_object.x_validation_feature[i].reshape(1,preprocess_object.max_len_text))\n",
        "      predicted_summaries.append(predicted_summary)\n",
        "\n",
        "      writer.writerow([i, acutual_summary, predicted_summary])\n",
        "      #print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "CPU times: user 23h 17min 30s, sys: 6h 3min 59s, total: 1d 5h 21min 29s\n",
            "Wall time: 3h 6min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCrdXFFFfjE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_summary = text_summary_model.decode_sequence(preprocess_object.x_validation_feature[1925].reshape(1,preprocess_object.max_len_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbotKlHTfocX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_summaries.append(predicted_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nQoVjbMe94U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_dataframe = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7xSQlJRfKPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_dataframe[\"actual_summaries\"] = actual_summaries\n",
        "prediction_dataframe[\"predicted_summaries\"] = predicted_summaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym1W6lxa2zCZ",
        "colab_type": "text"
      },
      "source": [
        "## **LSTM Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLkOiYTLTqJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = pd.read_csv(\"/content/drive/My Drive/Leuphana Projects/Text Summarization/predictions.csv\")\n",
        "\n",
        "results.columns = [\"index\",\"actual_summaries\", \"predicted_summaries\"]\n",
        "\n",
        "print(results.describe())\n",
        "\n",
        "import rouge\n",
        "\n",
        "class Evaluation:\n",
        "  def __init__(self, name, actual_summaries, predicted_summaries):\n",
        "    self.name = \"rouge\"\n",
        "    self.actual_summaries = actual_summaries\n",
        "    self.predicted_summaries = predicted_summaries\n",
        "    self.synonym_summaries = None\n",
        "\n",
        "  def rouge_score(self):\n",
        "    aggregator = \"Best\"\n",
        "    apply_avg = aggregator == 'Avg'\n",
        "    apply_best = aggregator == 'Best'\n",
        "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
        "                           max_n=4,\n",
        "                           limit_length=True,\n",
        "                           length_limit=100,\n",
        "                           length_limit_type='words',\n",
        "                           apply_avg=apply_avg,\n",
        "                           apply_best=apply_best,\n",
        "                           alpha=0.5, # Default F1_score\n",
        "                           weight_factor=1.2,\n",
        "                           stemming=True)\n",
        "    scores = evaluator.get_scores(predicted_summaries, actual_summaries)\n",
        "    print(scores)\n",
        "    return scores\n",
        "\n",
        "  def evaluate(self):\n",
        "    if self.name == \"rouge\":\n",
        "      self.rouge_score()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqtynLVZVs-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_object = Evaluation(\"rouge\", actual_summaries, predicted_summaries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pby0yEt1Vxah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_object.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}